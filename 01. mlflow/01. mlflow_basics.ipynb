{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ML flow settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install mlflow\n",
    "- mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # 꽃 받침과 꽃 잎 사이즈를 가지고 꽃의 종류를 결정\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 분리 => train_test_split()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "\n",
      "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "'sag', 'saga' and 'newton-cg' solvers.)\n",
      "\n",
      "This class implements regularized logistic regression using the\n",
      "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "that regularization is applied by default**. It can handle both dense\n",
      "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "floats for optimal performance; any other input format will be converted\n",
      "(and copied).\n",
      "\n",
      "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "with primal formulation, or no regularization. The 'liblinear' solver\n",
      "supports both L1 and L2 regularization, with a dual formulation only for\n",
      "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "'saga' solver.\n",
      "\n",
      "Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      "    Specify the norm of the penalty:\n",
      "\n",
      "    - `None`: no penalty is added;\n",
      "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "    - `'l1'`: add a L1 penalty term;\n",
      "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "\n",
      "    .. warning::\n",
      "       Some penalties may not work with some solvers. See the parameter\n",
      "       `solver` below, to know the compatibility between the penalty and\n",
      "       solver.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "\n",
      "dual : bool, default=False\n",
      "    Dual (constrained) or primal (regularized, see also\n",
      "    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      "    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      "    n_samples > n_features.\n",
      "\n",
      "tol : float, default=1e-4\n",
      "    Tolerance for stopping criteria.\n",
      "\n",
      "C : float, default=1.0\n",
      "    Inverse of regularization strength; must be a positive float.\n",
      "    Like in support vector machines, smaller values specify stronger\n",
      "    regularization.\n",
      "\n",
      "fit_intercept : bool, default=True\n",
      "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "    added to the decision function.\n",
      "\n",
      "intercept_scaling : float, default=1\n",
      "    Useful only when the solver 'liblinear' is used\n",
      "    and self.fit_intercept is set to True. In this case, x becomes\n",
      "    [x, self.intercept_scaling],\n",
      "    i.e. a \"synthetic\" feature with constant value equal to\n",
      "    intercept_scaling is appended to the instance vector.\n",
      "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "\n",
      "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "    as all other features.\n",
      "    To lessen the effect of regularization on synthetic feature weight\n",
      "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "\n",
      "class_weight : dict or 'balanced', default=None\n",
      "    Weights associated with classes in the form ``{class_label: weight}``.\n",
      "    If not given, all classes are supposed to have weight one.\n",
      "\n",
      "    The \"balanced\" mode uses the values of y to automatically adjust\n",
      "    weights inversely proportional to class frequencies in the input data\n",
      "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    Note that these weights will be multiplied with sample_weight (passed\n",
      "    through the fit method) if sample_weight is specified.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *class_weight='balanced'*\n",
      "\n",
      "random_state : int, RandomState instance, default=None\n",
      "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "    data. See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      "\n",
      "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "    To choose a solver, you might want to consider the following aspects:\n",
      "\n",
      "    - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "      and 'saga' are faster for large ones;\n",
      "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "      'lbfgs' handle multinomial loss;\n",
      "    - 'liblinear' and 'newton-cholesky' can only handle binary classification\n",
      "      by default. To apply a one-versus-rest scheme for the multiclass setting\n",
      "      one can wrapt it with the `OneVsRestClassifier`.\n",
      "    - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
      "      especially with one-hot encoded categorical features with rare\n",
      "      categories. Be aware that the memory usage of this solver has a quadratic\n",
      "      dependency on `n_features` because it explicitly computes the Hessian\n",
      "      matrix.\n",
      "\n",
      "    .. warning::\n",
      "       The choice of the algorithm depends on the penalty chosen and on\n",
      "       (multinomial) multiclass support:\n",
      "\n",
      "       ================= ============================== ======================\n",
      "       solver            penalty                        multinomial multiclass\n",
      "       ================= ============================== ======================\n",
      "       'lbfgs'           'l2', None                     yes\n",
      "       'liblinear'       'l1', 'l2'                     no\n",
      "       'newton-cg'       'l2', None                     yes\n",
      "       'newton-cholesky' 'l2', None                     no\n",
      "       'sag'             'l2', None                     yes\n",
      "       'saga'            'elasticnet', 'l1', 'l2', None yes\n",
      "       ================= ============================== ======================\n",
      "\n",
      "    .. note::\n",
      "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "       with approximately the same scale. You can preprocess the data with\n",
      "       a scaler from :mod:`sklearn.preprocessing`.\n",
      "\n",
      "    .. seealso::\n",
      "       Refer to the User Guide for more information regarding\n",
      "       :class:`LogisticRegression` and more specifically the\n",
      "       :ref:`Table <Logistic_regression>`\n",
      "       summarizing solver/penalty supports.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       Stochastic Average Gradient descent solver.\n",
      "    .. versionadded:: 0.19\n",
      "       SAGA solver.\n",
      "    .. versionchanged:: 0.22\n",
      "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "    .. versionadded:: 1.2\n",
      "       newton-cholesky solver.\n",
      "\n",
      "max_iter : int, default=100\n",
      "    Maximum number of iterations taken for the solvers to converge.\n",
      "\n",
      "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "    across the entire probability distribution, *even when the data is\n",
      "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "    and otherwise selects 'multinomial'.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "    .. versionchanged:: 0.22\n",
      "        Default changed from 'ovr' to 'auto' in 0.22.\n",
      "    .. deprecated:: 1.5\n",
      "       ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n",
      "       From then on, the recommended 'multinomial' will always be used for\n",
      "       `n_classes >= 3`.\n",
      "       Solvers that do not support 'multinomial' will raise an error.\n",
      "       Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n",
      "       still want to use OvR.\n",
      "\n",
      "verbose : int, default=0\n",
      "    For the liblinear and lbfgs solvers set verbose to any positive\n",
      "    number for verbosity.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to True, reuse the solution of the previous call to fit as\n",
      "    initialization, otherwise, just erase the previous solution.\n",
      "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    Number of CPU cores used when parallelizing over classes if\n",
      "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "    context. ``-1`` means using all processors.\n",
      "    See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "l1_ratio : float, default=None\n",
      "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "    combination of L1 and L2.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "\n",
      "classes_ : ndarray of shape (n_classes, )\n",
      "    A list of class labels known to the classifier.\n",
      "\n",
      "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "    Coefficient of the features in the decision function.\n",
      "\n",
      "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "\n",
      "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "    Intercept (a.k.a. bias) added to the decision function.\n",
      "\n",
      "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "    `intercept_` is of shape (1,) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "    outcome 0 (False).\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "    Actual number of iterations for all classes. If binary or multinomial,\n",
      "    it returns only 1 element. For liblinear solver, only the maximum\n",
      "    number of iteration across all classes is given.\n",
      "\n",
      "    .. versionchanged:: 0.20\n",
      "\n",
      "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "SGDClassifier : Incrementally trained logistic regression (when given\n",
      "    the parameter ``loss=\"log_loss\"``).\n",
      "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The underlying C implementation uses a random number generator to\n",
      "select features when fitting the model. It is thus not uncommon,\n",
      "to have slightly different results for the same input data. If\n",
      "that happens, try with a smaller tol parameter.\n",
      "\n",
      "Predict output may not match that of standalone liblinear in certain\n",
      "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "in the narrative documentation.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "\n",
      "LIBLINEAR -- A Library for Large Linear Classification\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "\n",
      "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "    https://hal.inria.fr/hal-00860051/document\n",
      "\n",
      "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      "\n",
      "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "    methods for logistic regression and maximum entropy models.\n",
      "    Machine Learning 85(1-2):41-75.\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import load_iris\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> X, y = load_iris(return_X_y=True)\n",
      ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      ">>> clf.predict(X[:2, :])\n",
      "array([0, 0])\n",
      ">>> clf.predict_proba(X[:2, :])\n",
      "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      ">>> clf.score(X, y)\n",
      "0.97...\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mlflow/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:42:49 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6475038260914396a152a4b00f8af678', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/Users/a-08/mlflow/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2024/08/14 15:42:51 INFO mlflow.tracking._tracking_service.client: 🏃 View run blushing-bat-446 at: http://127.0.0.1:5000/#/experiments/0/runs/6475038260914396a152a4b00f8af678.\n",
      "2024/08/14 15:42:51 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 93.33333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=0)\n",
    "model.fit(X_train, y_train) # train=모의고사 # 학습을 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "y_pred = model.predict(X_test) # 수능 문제를 제공\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도 : {accuracy * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Accuracy classification score.\n",
      "\n",
      "In multilabel classification, this function computes subset accuracy:\n",
      "the set of labels predicted for a sample must *exactly* match the\n",
      "corresponding set of labels in y_true.\n",
      "\n",
      "Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "    Ground truth (correct) labels.\n",
      "\n",
      "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "    Predicted labels, as returned by a classifier.\n",
      "\n",
      "normalize : bool, default=True\n",
      "    If ``False``, return the number of correctly classified samples.\n",
      "    Otherwise, return the fraction of correctly classified samples.\n",
      "\n",
      "sample_weight : array-like of shape (n_samples,), default=None\n",
      "    Sample weights.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "score : float or int\n",
      "    If ``normalize == True``, return the fraction of correctly\n",
      "    classified samples (float), else returns the number of correctly\n",
      "    classified samples (int).\n",
      "\n",
      "    The best performance is 1 with ``normalize == True`` and the number\n",
      "    of samples with ``normalize == False``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
      "    imbalanced datasets.\n",
      "jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "    two sets of samples.\n",
      "zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "    function will return the percentage of imperfectly predicted subsets.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.metrics import accuracy_score\n",
      ">>> y_pred = [0, 2, 1, 3]\n",
      ">>> y_true = [0, 1, 2, 3]\n",
      ">>> accuracy_score(y_true, y_pred)\n",
      "0.5\n",
      ">>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "2.0\n",
      "\n",
      "In the multilabel case with binary label indicators:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "0.5\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mlflow/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습과 모델 성능\n",
    "\n",
    "- 심플하게 모든 것을 ML flow에게 맡긴다. => mlflow.autolog()\n",
    "- autolog에서 추적하지 못하는 다른 파라미터, 메트릭, 메타데이터 등의 값을 수동으로 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(\"Tracking URI\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: iris_classification_experiment\n",
      "ID: 471758380148410695\n",
      "Location: mlflow-artifacts:/471758380148410695\n",
      "Tags: {}\n",
      "Lifecycle: active\n",
      "Create Timestamp: 1723613298348\n"
     ]
    }
   ],
   "source": [
    "exp = mlflow.set_experiment(experiment_name='iris_classification_experiment')\n",
    "\n",
    "print(f\"Name: {exp.name}\") # option + shift + 화살표 = copy\n",
    "print(f\"ID: {exp.experiment_id}\")\n",
    "print(f\"Location: {exp.artifact_location}\")\n",
    "print(f\"Tags: {exp.tags}\")\n",
    "print(f\"Lifecycle: {exp.lifecycle_stage}\")\n",
    "print(f\"Create Timestamp: {exp.creation_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1723617783.492173"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:43:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 15:43:09 INFO mlflow.tracking._tracking_service.client: 🏃 View run skittish-zebra-509 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/710be8818e364beba1e429da28f87f03.\n",
      "2024/08/14 15:43:09 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "mlflow.start_run() # 실험 시작\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train) # 학습 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"accuracy: {accuracy * 100}\")\n",
    "\n",
    "mlflow.end_run() # 실험 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:43:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 15:43:13 INFO mlflow.tracking._tracking_service.client: 🏃 View run nebulous-snail-327 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/832b9a0b327c437caa171fb82b87f65c.\n",
      "2024/08/14 15:43:13 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "# with, end 구문을 붙이지 않아도 알아서 실험 종료가 됩니다.\n",
    "with mlflow.start_run(nested=True): # 실험 시작\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train, y_train) # 학습 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"accuracy: {accuracy * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=200, # 최대 반복 횟수\n",
    "        C=1.0, # 규제 강도(C값이 작을수록 규제가 강해짐)\n",
    "        solver='lbfgs', # 최적화 알고리즘\n",
    "        random_state=123\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, # 트리의 갯수\n",
    "        max_depth=None,\n",
    "        random_state=123\n",
    "    ),\n",
    "    \"SVC\": SVC(\n",
    "        kernel='linear', # linear, sigmoid, poly, rbf\n",
    "        random_state=123 # 재연성을 위해 설정\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:43:19 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: LogisticRegression, Accuracy: 0.9666666666666667\n",
      "Model Name: RandomForest, Accuracy: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:44:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 7cfb7a2945ed494e836b9b08a219d534. Failed operations: [MlflowException(\"API request to http://127.0.0.1:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'127.0.0.1\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n",
      "2024/08/14 15:44:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run abundant-ram-650 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/7cfb7a2945ed494e836b9b08a219d534.\n",
      "2024/08/14 15:44:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: SVC, Accuracy: 0.9333333333333333\n",
      "Best Model Name: LogisticRegression, Best Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 위 모델들을 한번에 불러와서(반복문) => 최고의 모델을 찾아내고, 해당 파라미터를 기록합니다.\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "with mlflow.start_run():\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Model Name: {model_name}, Accuracy: {accuracy}\")\n",
    "\n",
    "        mlflow.log_param('best_model', best_model_name) # 파라미터 로그\n",
    "        mlflow.log_metric('best_accuracy', best_accuracy) # 메트릭 로그\n",
    "\n",
    "    print(f\"Best Model Name: {best_model_name}, Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:44:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 15:44:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 15:44:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run LogisticRegression at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/a70a5e6a71b145aa8e8cf653cf89b8ae.\n",
      "2024/08/14 15:44:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: LogisticRegression, Accuracy: 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:44:33 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 15:44:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run RandomForest at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/d11e5136b2354258a2f0bbe3f506a2ab.\n",
      "2024/08/14 15:44:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: RandomForest, Accuracy: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 15:44:35 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 15:44:35 INFO mlflow.tracking._tracking_service.client: 🏃 View run SVC at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/2861678abfdb42929393ba0fb2e53746.\n",
      "2024/08/14 15:44:35 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: SVC, Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "# 전체 모델에 대해서 기록을 하고 싶은데?\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # model\n",
    "        model_path = f\"{model_name}_model\"\n",
    "        mlflow.sklearn.log_model(model, model_path) # 모델을 artifact 디렉토리에 저장\n",
    "\n",
    "        mlflow.log_param(f'{model_name}_param', model.get_params()) # 파라미터 로그\n",
    "        mlflow.log_metric(f'{model_name}_accuracy', accuracy) # 메트릭 로그\n",
    "\n",
    "    print(f\"Model Name: {model_name}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "def promote_to_staging(): pass # stage\n",
    "def promote_to_production(): pass # production\n",
    "def archive_model(): pass # archive: 모델 폐기 단계\n",
    "def register_model(): pass # 모델 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
