{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ML flow settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install mlflow\n",
    "- mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # 꽃 받침과 꽃 잎 사이즈를 가지고 꽃의 종류를 결정\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 분리 => train_test_split()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "\n",
      "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "'sag', 'saga' and 'newton-cg' solvers.)\n",
      "\n",
      "This class implements regularized logistic regression using the\n",
      "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "that regularization is applied by default**. It can handle both dense\n",
      "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "floats for optimal performance; any other input format will be converted\n",
      "(and copied).\n",
      "\n",
      "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "with primal formulation, or no regularization. The 'liblinear' solver\n",
      "supports both L1 and L2 regularization, with a dual formulation only for\n",
      "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "'saga' solver.\n",
      "\n",
      "Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      "    Specify the norm of the penalty:\n",
      "\n",
      "    - `None`: no penalty is added;\n",
      "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "    - `'l1'`: add a L1 penalty term;\n",
      "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "\n",
      "    .. warning::\n",
      "       Some penalties may not work with some solvers. See the parameter\n",
      "       `solver` below, to know the compatibility between the penalty and\n",
      "       solver.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "\n",
      "dual : bool, default=False\n",
      "    Dual (constrained) or primal (regularized, see also\n",
      "    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      "    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      "    n_samples > n_features.\n",
      "\n",
      "tol : float, default=1e-4\n",
      "    Tolerance for stopping criteria.\n",
      "\n",
      "C : float, default=1.0\n",
      "    Inverse of regularization strength; must be a positive float.\n",
      "    Like in support vector machines, smaller values specify stronger\n",
      "    regularization.\n",
      "\n",
      "fit_intercept : bool, default=True\n",
      "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "    added to the decision function.\n",
      "\n",
      "intercept_scaling : float, default=1\n",
      "    Useful only when the solver 'liblinear' is used\n",
      "    and self.fit_intercept is set to True. In this case, x becomes\n",
      "    [x, self.intercept_scaling],\n",
      "    i.e. a \"synthetic\" feature with constant value equal to\n",
      "    intercept_scaling is appended to the instance vector.\n",
      "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "\n",
      "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "    as all other features.\n",
      "    To lessen the effect of regularization on synthetic feature weight\n",
      "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "\n",
      "class_weight : dict or 'balanced', default=None\n",
      "    Weights associated with classes in the form ``{class_label: weight}``.\n",
      "    If not given, all classes are supposed to have weight one.\n",
      "\n",
      "    The \"balanced\" mode uses the values of y to automatically adjust\n",
      "    weights inversely proportional to class frequencies in the input data\n",
      "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    Note that these weights will be multiplied with sample_weight (passed\n",
      "    through the fit method) if sample_weight is specified.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *class_weight='balanced'*\n",
      "\n",
      "random_state : int, RandomState instance, default=None\n",
      "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "    data. See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      "\n",
      "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "    To choose a solver, you might want to consider the following aspects:\n",
      "\n",
      "    - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "      and 'saga' are faster for large ones;\n",
      "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "      'lbfgs' handle multinomial loss;\n",
      "    - 'liblinear' and 'newton-cholesky' can only handle binary classification\n",
      "      by default. To apply a one-versus-rest scheme for the multiclass setting\n",
      "      one can wrapt it with the `OneVsRestClassifier`.\n",
      "    - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
      "      especially with one-hot encoded categorical features with rare\n",
      "      categories. Be aware that the memory usage of this solver has a quadratic\n",
      "      dependency on `n_features` because it explicitly computes the Hessian\n",
      "      matrix.\n",
      "\n",
      "    .. warning::\n",
      "       The choice of the algorithm depends on the penalty chosen and on\n",
      "       (multinomial) multiclass support:\n",
      "\n",
      "       ================= ============================== ======================\n",
      "       solver            penalty                        multinomial multiclass\n",
      "       ================= ============================== ======================\n",
      "       'lbfgs'           'l2', None                     yes\n",
      "       'liblinear'       'l1', 'l2'                     no\n",
      "       'newton-cg'       'l2', None                     yes\n",
      "       'newton-cholesky' 'l2', None                     no\n",
      "       'sag'             'l2', None                     yes\n",
      "       'saga'            'elasticnet', 'l1', 'l2', None yes\n",
      "       ================= ============================== ======================\n",
      "\n",
      "    .. note::\n",
      "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "       with approximately the same scale. You can preprocess the data with\n",
      "       a scaler from :mod:`sklearn.preprocessing`.\n",
      "\n",
      "    .. seealso::\n",
      "       Refer to the User Guide for more information regarding\n",
      "       :class:`LogisticRegression` and more specifically the\n",
      "       :ref:`Table <Logistic_regression>`\n",
      "       summarizing solver/penalty supports.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       Stochastic Average Gradient descent solver.\n",
      "    .. versionadded:: 0.19\n",
      "       SAGA solver.\n",
      "    .. versionchanged:: 0.22\n",
      "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "    .. versionadded:: 1.2\n",
      "       newton-cholesky solver.\n",
      "\n",
      "max_iter : int, default=100\n",
      "    Maximum number of iterations taken for the solvers to converge.\n",
      "\n",
      "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "    across the entire probability distribution, *even when the data is\n",
      "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "    and otherwise selects 'multinomial'.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "    .. versionchanged:: 0.22\n",
      "        Default changed from 'ovr' to 'auto' in 0.22.\n",
      "    .. deprecated:: 1.5\n",
      "       ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n",
      "       From then on, the recommended 'multinomial' will always be used for\n",
      "       `n_classes >= 3`.\n",
      "       Solvers that do not support 'multinomial' will raise an error.\n",
      "       Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n",
      "       still want to use OvR.\n",
      "\n",
      "verbose : int, default=0\n",
      "    For the liblinear and lbfgs solvers set verbose to any positive\n",
      "    number for verbosity.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to True, reuse the solution of the previous call to fit as\n",
      "    initialization, otherwise, just erase the previous solution.\n",
      "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    Number of CPU cores used when parallelizing over classes if\n",
      "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "    context. ``-1`` means using all processors.\n",
      "    See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "l1_ratio : float, default=None\n",
      "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "    combination of L1 and L2.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "\n",
      "classes_ : ndarray of shape (n_classes, )\n",
      "    A list of class labels known to the classifier.\n",
      "\n",
      "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "    Coefficient of the features in the decision function.\n",
      "\n",
      "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "\n",
      "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "    Intercept (a.k.a. bias) added to the decision function.\n",
      "\n",
      "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "    `intercept_` is of shape (1,) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "    outcome 0 (False).\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "    Actual number of iterations for all classes. If binary or multinomial,\n",
      "    it returns only 1 element. For liblinear solver, only the maximum\n",
      "    number of iteration across all classes is given.\n",
      "\n",
      "    .. versionchanged:: 0.20\n",
      "\n",
      "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "SGDClassifier : Incrementally trained logistic regression (when given\n",
      "    the parameter ``loss=\"log_loss\"``).\n",
      "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The underlying C implementation uses a random number generator to\n",
      "select features when fitting the model. It is thus not uncommon,\n",
      "to have slightly different results for the same input data. If\n",
      "that happens, try with a smaller tol parameter.\n",
      "\n",
      "Predict output may not match that of standalone liblinear in certain\n",
      "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "in the narrative documentation.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "\n",
      "LIBLINEAR -- A Library for Large Linear Classification\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "\n",
      "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "    https://hal.inria.fr/hal-00860051/document\n",
      "\n",
      "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      "\n",
      "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "    methods for logistic regression and maximum entropy models.\n",
      "    Machine Learning 85(1-2):41-75.\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import load_iris\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> X, y = load_iris(return_X_y=True)\n",
      ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      ">>> clf.predict(X[:2, :])\n",
      "array([0, 0])\n",
      ">>> clf.predict_proba(X[:2, :])\n",
      "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      ">>> clf.score(X, y)\n",
      "0.97...\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mlflow/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 93.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a-08/mlflow/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=0)\n",
    "model.fit(X_train, y_train) # train=모의고사 # 학습을 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "y_pred = model.predict(X_test) # 수능 문제를 제공\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도 : {accuracy * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습과 모델 성능\n",
    "\n",
    "- 심플하게 모든 것을 ML flow에게 맡긴다. => mlflow.autolog()\n",
    "- autolog에서 추적하지 못하는 다른 파라미터, 메트릭, 메타데이터 등의 값을 수동으로 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(\"Tracking URI\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: iris_classification_experiment\n",
      "ID: 471758380148410695\n",
      "Location: mlflow-artifacts:/471758380148410695\n",
      "Tags: {}\n",
      "Lifecycle: active\n",
      "Create Timestamp: 1723613298348\n"
     ]
    }
   ],
   "source": [
    "exp = mlflow.set_experiment(experiment_name='iris_classification_experiment')\n",
    "\n",
    "print(f\"Name: {exp.name}\") # option + shift + 화살표 = copy\n",
    "print(f\"ID: {exp.experiment_id}\")\n",
    "print(f\"Location: {exp.artifact_location}\")\n",
    "print(f\"Tags: {exp.tags}\")\n",
    "print(f\"Lifecycle: {exp.lifecycle_stage}\")\n",
    "print(f\"Create Timestamp: {exp.creation_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1723625878.5851882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:57:58 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 17:58:00 INFO mlflow.tracking._tracking_service.client: 🏃 View run abrasive-snake-442 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/2aa88d9534004c08a27bec3df522ba66.\n",
      "2024/08/14 17:58:00 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "mlflow.start_run() # 실험 시작\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train) # 학습 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"accuracy: {accuracy * 100}\")\n",
    "\n",
    "mlflow.end_run() # 실험 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:58:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 17:58:02 INFO mlflow.tracking._tracking_service.client: 🏃 View run upset-skink-597 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/3d7ab2aa8d7e456688dcbefa3dff30d1.\n",
      "2024/08/14 17:58:02 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "# with, end 구문을 붙이지 않아도 알아서 실험 종료가 됩니다.\n",
    "with mlflow.start_run(nested=True): # 실험 시작\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train, y_train) # 학습 시킬 때는 학습 데이터만 제공\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"accuracy: {accuracy * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=200, # 최대 반복 횟수\n",
    "        C=1.0, # 규제 강도(C값이 작을수록 규제가 강해짐)\n",
    "        solver='lbfgs', # 최적화 알고리즘\n",
    "        random_state=123\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, # 트리의 갯수\n",
    "        max_depth=None,\n",
    "        random_state=123\n",
    "    ),\n",
    "    \"SVC\": SVC(\n",
    "        kernel='linear', # linear, sigmoid, poly, rbf\n",
    "        random_state=123 # 재연성을 위해 설정\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:58:02 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: LogisticRegression, Accuracy: 0.9666666666666667\n",
      "Model Name: RandomForest, Accuracy: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:59:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 89f4341fff234e7fad0898858514490e. Failed operations: [MlflowException(\"API request to http://127.0.0.1:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'127.0.0.1\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n",
      "2024/08/14 17:59:06 INFO mlflow.tracking._tracking_service.client: 🏃 View run industrious-hare-447 at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/89f4341fff234e7fad0898858514490e.\n",
      "2024/08/14 17:59:06 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: SVC, Accuracy: 0.9333333333333333\n",
      "Best Model Name: LogisticRegression, Best Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 위 모델들을 한번에 불러와서(반복문) => 최고의 모델을 찾아내고, 해당 파라미터를 기록합니다.\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "with mlflow.start_run():\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Model Name: {model_name}, Accuracy: {accuracy}\")\n",
    "\n",
    "        mlflow.log_param('best_model', best_model_name) # 파라미터 로그\n",
    "        mlflow.log_metric('best_accuracy', best_accuracy) # 메트릭 로그\n",
    "\n",
    "    print(f\"Best Model Name: {best_model_name}, Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:59:06 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/14 17:59:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 17:59:08 INFO mlflow.tracking._tracking_service.client: 🏃 View run LogisticRegression at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/53a83d5e40c44e4e8c9bac7998fed68b.\n",
      "2024/08/14 17:59:08 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: LogisticRegression, Accuracy: 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:59:10 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 17:59:11 INFO mlflow.tracking._tracking_service.client: 🏃 View run RandomForest at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/1b063b7b3f8c40bab3689a577c529231.\n",
      "2024/08/14 17:59:11 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: RandomForest, Accuracy: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/14 17:59:13 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/14 17:59:13 INFO mlflow.tracking._tracking_service.client: 🏃 View run SVC at: http://127.0.0.1:5000/#/experiments/471758380148410695/runs/c7b4c1e5ef134c12951c4f21b7552207.\n",
      "2024/08/14 17:59:13 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/471758380148410695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: SVC, Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "# 전체 모델에 대해서 기록을 하고 싶은데?\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # model\n",
    "        model_path = f\"{model_name}_model\"\n",
    "        mlflow.sklearn.log_model(model, model_path) # 모델을 artifact 디렉토리에 저장\n",
    "\n",
    "        mlflow.log_param(f'{model_name}_param', model.get_params()) # 파라미터 로그\n",
    "        mlflow.log_metric(f'{model_name}_accuracy', accuracy) # 메트릭 로그\n",
    "\n",
    "    print(f\"Model Name: {model_name}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# 모델을 등록하고, 해당 모델의 버전을 반환\n",
    "def register_model(model_name,run_id, model_uri='model'): # 모델 등록\n",
    "    model_uri = f'runs:/{run_id}/{model_uri}'\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    return model_version # json 형태\n",
    "\n",
    "# 등록된 모델을 stage 단계로 승격\n",
    "def promote_to_staging(model_name, run_id, model_uri): # stage\n",
    "    model_version = register_model(model_name, run_id, model_uri)\n",
    "\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version, # json을 변환\n",
    "        key='stage',\n",
    "        value='staging'\n",
    "    )\n",
    "\n",
    "    print(f'Model: {model_name}, Version: {model_version} promoted to Staging...')\n",
    "\n",
    "def promote_to_production(model_name, version):  # production, 눈으로 보고 판단해야해서 버전을 직접 입력함\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        key='stage',\n",
    "        value='production'\n",
    "    )\n",
    "\n",
    "    print(f'Model: {model_name}, Version: {version} promoted to Production...')\n",
    "\n",
    "def archive_model(model_name, version):  # archive: 모델 폐기 단계, 눈으로 보고 판단해야해서 버전을 직접 입력함\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        key='stage',\n",
    "        value='archived'\n",
    "    )\n",
    "\n",
    "    print(f'Model: {model_name}, Version: {version} Archived...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LogisticRegression' already exists. Creating a new version of this model...\n",
      "2024/08/14 17:59:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: aliases=[], creation_timestamp=1723625953244, current_stage='None', description='', last_updated_timestamp=1723625953244, name='LogisticRegression', run_id='922ddd6e306d4849be80101808e94cd6', run_link='', source='mlflow-artifacts:/0/922ddd6e306d4849be80101808e94cd6/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='9'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '9' of model 'LogisticRegression'.\n"
     ]
    }
   ],
   "source": [
    "# http://127.0.0.1:5000/#/experiments/0/runs/922ddd6e306d4849be80101808e94cd6\n",
    "# 실험 ID: 0\n",
    "# 실행 ID: 922ddd6e306d4849be80101808e94cd6\n",
    "# Model Name: LogisticRegression\n",
    "\n",
    "# (1) 모델 등록\n",
    "run_id = '922ddd6e306d4849be80101808e94cd6'\n",
    "model_name = 'LogisticRegression'\n",
    "\n",
    "model_version = register_model(model_name, run_id)\n",
    "print(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LogisticRegression' already exists. Creating a new version of this model...\n",
      "2024/08/14 17:59:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Version: <ModelVersion: aliases=[], creation_timestamp=1723625953265, current_stage='None', description='', last_updated_timestamp=1723625953265, name='LogisticRegression', run_id='922ddd6e306d4849be80101808e94cd6', run_link='', source='mlflow-artifacts:/0/922ddd6e306d4849be80101808e94cd6/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='10'> promoted to Staging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LogisticRegression'.\n"
     ]
    }
   ],
   "source": [
    "# (2) 모델을 staging 단계로 승격\n",
    "promote_to_staging(model_name, run_id, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Version: 3 promoted to Production...\n"
     ]
    }
   ],
   "source": [
    "# (3) 모델을 Production 단계로 승격\n",
    "promote_to_production(model_name, '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Version: 4 promoted to Production...\n",
      "Model: LogisticRegression, Version: 3 Archived...\n"
     ]
    }
   ],
   "source": [
    "# (4) 새로운 버전의 모델을 Production 단계로 승격\n",
    "promote_to_production(model_name, '4')\n",
    "\n",
    "# (4) 기존 Production 버전의 모델을 archive 단계로 폐기\n",
    "archive_model(model_name, '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 Serving\n",
    "\n",
    "- Fast API, Flask ... => API로 어떻게 만들지?\n",
    "- mlflow가 해결\n",
    "- inference: 값을 전달하고, 그 값에 대한 예측값을 return (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Model Load\n",
    "model_name = 'LogisticRegression'\n",
    "model_version = 1\n",
    "\n",
    "model_uri = f'models:/{model_name}/{model_version}'\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "test_input = X_test[:10]\n",
    "loaded_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.53333275e-01, -1.28296331e+00,  6.49083415e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.15917263e+00, -1.31979479e-01,  9.90107977e-01,\n",
       "         1.18556721e+00],\n",
       "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
       "         1.31719939e+00],\n",
       "       [-2.94841818e-01, -1.31979479e-01,  1.94384000e-01,\n",
       "         1.32509732e-01],\n",
       "       [-1.14301691e+00,  1.24920112e+00, -1.34022653e+00,\n",
       "        -1.44707648e+00],\n",
       "       [ 1.89829664e-01, -1.31979479e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 5.53333275e-01, -1.74335684e+00,  3.64896281e-01,\n",
       "         1.32509732e-01],\n",
       "       [-1.74885626e+00,  3.28414053e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.74885626e+00, -3.62176246e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-4.16009689e-01, -1.05276654e+00,  3.64896281e-01,\n",
       "         8.77547895e-04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model API Serving\n",
    "\n",
    "- 서버가 하나 더 필요 => REST API\n",
    "- mlflow 설치할 때 flask => API를 내려줄 flask 서버를 하나 더 띄워야 함\n",
    "\n",
    "http://127.0.0.1:5000/#/experiments/0/runs/1b74db9e2ab44b41a35cbc26a09baf7f\n",
    "\n",
    "mlflow models serve -m ./mlartifacts/0/1b74db9e2ab44b41a35cbc26a09baf7f/artifacts/model -p 5001 --no-conda\n",
    "=> 로컬에서 돌리고 있는데, 추후 AWS Sage Maker에 올려서 운영 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server Response(inference) :  {'predictions': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n",
    "\n",
    "data = {\n",
    "    'dataframe_split': X_test_df[:10].to_dict(orient=\"split\")\n",
    "} # dict\n",
    "\n",
    "url = 'http://127.0.0.1:5001/invocations'\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "res = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(\"Server Response(inference) : \", res.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
